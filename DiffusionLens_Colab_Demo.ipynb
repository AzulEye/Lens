{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "header"
   },
   "source": [
    "# DiffusionLens: GPU-Accelerated Demo\n",
    "\n",
    "This notebook demonstrates the DiffusionLens method for analyzing text encoders in diffusion models.\n",
    "\n",
    "**Paper**: [Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines](https://arxiv.org/abs/2403.05846)\n",
    "\n",
    "**What this does**: Extracts and visualizes intermediate layer representations from text encoders, showing how concepts progressively form across layers.\n",
    "\n",
    "**Requirements**: \n",
    "- GPU runtime (Runtime → Change runtime type → GPU)\n",
    "- ~15GB disk space for models\n",
    "- 10-20 minutes for full demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "setup_header"
   },
   "source": [
    "## 1. Setup: Clone Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "clone_repo"
   },
   "outputs": [],
   "source": [
    "# Clone the repository\n",
    "!git clone https://github.com/AzulEye/Lens.git\n",
    "%cd Lens\n",
    "\n",
    "# Verify we're in the right directory\n",
    "!ls -la"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "deps_header"
   },
   "source": [
    "## 2. Install Dependencies\n",
    "\n",
    "**Strategy**: Use pre-built wheels to avoid compilation issues, and install only necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "install_deps"
   },
   "outputs": [],
   "source": [
    "# Install dependencies that work with Colab\n",
    "# Using versions that have pre-built wheels and don't require compilation\n",
    "print(\"Installing compatible packages for Colab...\\n\")\n",
    "\n",
    "# Install tokenizers first (pre-built version)\n",
    "!pip install -q tokenizers==0.13.3\n",
    "\n",
    "# Install transformers with compatible tokenizers\n",
    "!pip install -q transformers==4.30.2 --no-deps\n",
    "!pip install -q filelock huggingface-hub pyyaml regex requests packaging\n",
    "\n",
    "# Install other core dependencies\n",
    "!pip install -q matplotlib python-box\n",
    "\n",
    "print(\"\\n✅ Core dependencies installed\")\n",
    "print(\"\\nInstalled versions:\")\n",
    "!pip show transformers tokenizers | grep -E '^Name:|^Version:'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "diffusers_header"
   },
   "source": [
    "## 3. Setup Modified Diffusers (diffusers_local)\n",
    "\n",
    "The DiffusionLens method uses modified diffusion pipelines that support layer-wise text encoder extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "setup_diffusers"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Download diffusers v0.20.2 (the version with our modifications)\n",
    "!wget -q https://github.com/huggingface/diffusers/archive/refs/tags/v0.20.2.tar.gz\n",
    "!tar -xzf v0.20.2.tar.gz\n",
    "!mv diffusers-0.20.2 diffusers_local\n",
    "\n",
    "# Copy our modified pipeline files\n",
    "!cp pipeline_stable_diffusion.py diffusers_local/src/diffusers/pipelines/stable_diffusion/\n",
    "!cp pipeline_if.py diffusers_local/src/diffusers/pipelines/deepfloyd_if/\n",
    "\n",
    "# Verify the setup\n",
    "if os.path.exists('diffusers_local/src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py'):\n",
    "    print(\"✅ diffusers_local setup complete\")\n",
    "else:\n",
    "    print(\"❌ Error: pipeline files not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "verify_header"
   },
   "source": [
    "## 4. Verify GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "verify_gpu"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA version: {torch.version.cuda}\")\n",
    "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(\"\\n✅ GPU ready for fast inference!\")\n",
    "else:\n",
    "    print(\"\\n⚠️  Warning: No GPU detected. Please enable GPU in Runtime → Change runtime type\")\n",
    "    print(\"The demo will run very slowly on CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "demo_header"
   },
   "source": [
    "## 5. Run DiffusionLens Demo\n",
    "\n",
    "This will generate images showing how the text encoder progressively builds understanding across layers.\n",
    "\n",
    "**Parameters:**\n",
    "- Model: Stable Diffusion 1.4 (4GB, fast)\n",
    "- Prompts: Authentic examples from the paper\n",
    "- Layers: 0, 4, 8, 12 (early → mid → late → final)\n",
    "- Images per layer: 1 (faster demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "run_demo"
   },
   "outputs": [],
   "source": [
    "# Run the demo with paper prompts\n",
    "!python run_experiment.py \\\n",
    "    --model_key sd1.4 \\\n",
    "    --img_num 1 \\\n",
    "    --generate \\\n",
    "    --start_layer 0 \\\n",
    "    --end_layer 12 \\\n",
    "    --step_layer 4 \\\n",
    "    --folder_name colab_demo \\\n",
    "    --input_filename paper_prompts.txt \\\n",
    "    --number_of_inputs 2\n",
    "\n",
    "print(\"\\n✅ Demo complete! Check the output below.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "visualize_header"
   },
   "source": [
    "## 6. Visualize Results\n",
    "\n",
    "Display the generated images showing layer-by-layer concept formation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "display_results"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Read the prompts we used\n",
    "with open('inputs/paper_prompts.txt', 'r') as f:\n",
    "    prompts = [line.strip() for line in f.readlines()[:2]]  # First 2 prompts\n",
    "\n",
    "# Display results for each prompt\n",
    "for prompt in prompts:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    base_path = Path(f'colab_demo/{prompt}/sd1.4/encoder_full_direct')\n",
    "    \n",
    "    if not base_path.exists():\n",
    "        print(f\"⚠️  Output not found for: {prompt}\")\n",
    "        continue\n",
    "    \n",
    "    # Find all layer images\n",
    "    layer_images = sorted(base_path.glob('layer_*.png'))\n",
    "    \n",
    "    if not layer_images:\n",
    "        print(f\"⚠️  No layer images found for: {prompt}\")\n",
    "        continue\n",
    "    \n",
    "    # Display in a grid\n",
    "    num_layers = len(layer_images)\n",
    "    fig, axes = plt.subplots(1, num_layers, figsize=(5*num_layers, 5))\n",
    "    \n",
    "    if num_layers == 1:\n",
    "        axes = [axes]\n",
    "    \n",
    "    for idx, img_path in enumerate(layer_images):\n",
    "        img = Image.open(img_path)\n",
    "        layer_num = img_path.stem.split('_')[1]  # Extract layer number from filename\n",
    "        \n",
    "        axes[idx].imshow(img)\n",
    "        axes[idx].set_title(f'Layer {layer_num}', fontsize=14, fontweight='bold')\n",
    "        axes[idx].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"\\nLayer progression:\")\n",
    "    print(\"  Layer 0:  Early - abstract concepts, minimal semantic understanding\")\n",
    "    print(\"  Layer 4:  Mid-early - concepts starting to form\")\n",
    "    print(\"  Layer 8:  Mid-late - concepts interacting and composing\")\n",
    "    print(\"  Layer 12: Final - complete understanding and composition\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "explore_header"
   },
   "source": [
    "## 7. Explore Individual Layers (Optional)\n",
    "\n",
    "You can also view individual images from the `all_images/` directory for more detailed analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "explore_layers"
   },
   "outputs": [],
   "source": [
    "# Example: Show all individual layer images for the first prompt\n",
    "from IPython.display import display\n",
    "\n",
    "prompt = prompts[0]\n",
    "print(f\"Detailed view for: {prompt}\\n\")\n",
    "\n",
    "all_images_path = Path(f'colab_demo/{prompt}/sd1.4/encoder_full_direct/all_images')\n",
    "\n",
    "if all_images_path.exists():\n",
    "    images = sorted(all_images_path.glob('*.png'))\n",
    "    \n",
    "    for img_path in images:\n",
    "        print(f\"\\n{img_path.name}:\")\n",
    "        img = Image.open(img_path)\n",
    "        display(img)\n",
    "else:\n",
    "    print(\"No detailed images found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "download_header"
   },
   "source": [
    "## 8. Download Results (Optional)\n",
    "\n",
    "Download all generated images as a zip file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "download_results"
   },
   "outputs": [],
   "source": [
    "# Create a zip file of all results\n",
    "!zip -r colab_demo_results.zip colab_demo/\n",
    "\n",
    "print(\"\\n✅ Results zipped!\")\n",
    "print(\"To download: Click the folder icon on the left, find 'colab_demo_results.zip', right-click → Download\")\n",
    "\n",
    "# Alternative: Direct download link (may not work in all browsers)\n",
    "from google.colab import files\n",
    "files.download('colab_demo_results.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "custom_header"
   },
   "source": [
    "## 9. Run with Custom Prompts (Optional)\n",
    "\n",
    "Try your own prompts to see how the text encoder builds understanding!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "custom_prompts"
   },
   "outputs": [],
   "source": [
    "# Define your custom prompts\n",
    "custom_prompts = [\n",
    "    \"A robot painting a self-portrait\",\n",
    "    \"An elephant balancing on a ball\",\n",
    "]\n",
    "\n",
    "# Write to file\n",
    "with open('inputs/custom_prompts.txt', 'w') as f:\n",
    "    for prompt in custom_prompts:\n",
    "        f.write(prompt + '\\n')\n",
    "\n",
    "# Run the experiment\n",
    "!python run_experiment.py \\\n",
    "    --model_key sd1.4 \\\n",
    "    --img_num 1 \\\n",
    "    --generate \\\n",
    "    --start_layer 0 \\\n",
    "    --end_layer 12 \\\n",
    "    --step_layer 4 \\\n",
    "    --folder_name custom_demo \\\n",
    "    --input_filename custom_prompts.txt \\\n",
    "    --number_of_inputs 2\n",
    "\n",
    "print(\"\\n✅ Custom prompts complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "info_header"
   },
   "source": [
    "## Understanding the Results\n",
    "\n",
    "**What you're seeing:**\n",
    "\n",
    "Each image shows what the diffusion model generates when it only has access to information up to that layer of the text encoder.\n",
    "\n",
    "**Layer progression patterns:**\n",
    "\n",
    "1. **Layer 0 (Early)**: Very abstract, minimal semantic understanding\n",
    "   - Generic shapes and colors\n",
    "   - No clear object identity\n",
    "\n",
    "2. **Layer 4 (Mid-early)**: Concepts starting to emerge\n",
    "   - Individual objects becoming recognizable\n",
    "   - Limited interaction between concepts\n",
    "\n",
    "3. **Layer 8 (Mid-late)**: Composition forming\n",
    "   - Multiple concepts interacting\n",
    "   - Spatial relationships developing\n",
    "\n",
    "4. **Layer 12 (Final)**: Complete understanding\n",
    "   - Full scene composition\n",
    "   - All concepts properly integrated\n",
    "   - Highest quality and coherence\n",
    "\n",
    "**Paper findings:**\n",
    "- **Compositional understanding** builds progressively (e.g., \"A cake on a butterfly\")\n",
    "- **Rare concepts** (e.g., \"babirusa\") require more layers to retrieve correctly\n",
    "- **Complex scenes** show clearer layer-by-layer progression than simple objects\n",
    "\n",
    "## References\n",
    "\n",
    "- **Paper**: [Diffusion Lens: Interpreting Text Encoders in Text-to-Image Pipelines](https://arxiv.org/abs/2403.05846)\n",
    "- **Original Repository**: [github.com/tokeron/DiffusionLens](https://github.com/tokeron/DiffusionLens)\n",
    "- **This Fork**: [github.com/AzulEye/Lens](https://github.com/AzulEye/Lens)\n",
    "\n",
    "## Tips\n",
    "\n",
    "- **Faster runtime**: Reduce `--img_num` to 1, increase `--step_layer` to 6\n",
    "- **Better quality**: Increase `--img_num` to 4 (generates 4 images per layer)\n",
    "- **Different model**: Try `--model_key sd2.1` (larger model, better quality, slower)\n",
    "- **More prompts**: Increase `--number_of_inputs` to process more from the file\n",
    "\n",
    "## Troubleshooting\n",
    "\n",
    "**Out of memory?**\n",
    "- Restart runtime and try again\n",
    "- Reduce `--img_num` to 1\n",
    "- Use `sd1.4` instead of larger models\n",
    "\n",
    "**Slow generation?**\n",
    "- Verify GPU is enabled (see Section 4)\n",
    "- Expected: ~2-3 minutes per prompt with GPU, ~30-60 minutes on CPU\n",
    "\n",
    "**Tokenizers build error?**\n",
    "- Section 2 now uses pre-built tokenizers wheel (v0.13.3)\n",
    "- If issues persist, restart runtime and re-run from Section 1"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
